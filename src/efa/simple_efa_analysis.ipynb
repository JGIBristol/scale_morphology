{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c117ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, read in the data\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "from scale_morphology.scales import util\n",
    "\n",
    "# Put the directory containing segmented scales here\n",
    "# Here I've read directly from the RDSF\n",
    "parent_dir = (\n",
    "    pathlib.Path(util.config()[\"rdsf_mount\"]) / \"Carran/Postgrad/segmentations_cleaned\"\n",
    ").expanduser()\n",
    "assert parent_dir.exists()\n",
    "\n",
    "segmentation_paths = sorted([str(x) for x in parent_dir.glob(\"*.tif\")])\n",
    "f\"{len(segmentation_paths)} segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in metadata, including image filepaths\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scale_morphology.scales import metadata\n",
    "\n",
    "df = metadata.df([str(x) for x in segmentation_paths])\n",
    "df = df.drop(columns=\"no_scale\")\n",
    "\n",
    "assert len(df) == 928, \"Did the number of scales change?\"\n",
    "print(len(df), \"scales after dropping empty ones\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e2a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If necessary, read in the scales from the RDSF and perform EFA on them.\n",
    "\n",
    "Otherwise read in the EFA coefficients from a cache\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scale_morphology.scales import efa\n",
    "\n",
    "\n",
    "coeff_dump = pathlib.Path(\"efa_coeffs.npy\")\n",
    "\n",
    "if coeff_dump.is_file():\n",
    "    coeffs = np.load(coeff_dump)\n",
    "else:\n",
    "    n_edge_points, order = 300, 50\n",
    "    coeffs = efa.run_analysis(\n",
    "        df[\"path\"],\n",
    "        df[\"magnification\"],\n",
    "        n_points=n_edge_points,\n",
    "        order=order,\n",
    "        n_threads=32,\n",
    "    )\n",
    "\n",
    "    np.save(coeff_dump, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose which scales to keep, and the criteria to group the scales by\n",
    "\"\"\"\n",
    "\n",
    "# This tells us we want to keep:\n",
    "# - scales where sex is NOT ?\n",
    "# - scales where growth is infinite (i.e. onto)\n",
    "# - scales where mutation is WT\n",
    "keep = (df[\"sex\"] != \"?\") & (df[\"growth\"] == np.inf) & (df[\"mutation\"] == \"WT\")\n",
    "filtered_df = df[keep]\n",
    "\n",
    "# Choose which category to group by when we do the LDA and colour-coded plotting\n",
    "categories = [\"sex\", \"age\"]\n",
    "\n",
    "grouping_df = filtered_df.loc[keep, categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform PCA\n",
    "\n",
    "We won't scale the features before PCA, because we want them to keep their original\n",
    "magnitudes - if we scale the features to have a standard mean/std, then we will\n",
    "artificially inflate the importance of the higher harmonics.\n",
    "\n",
    "I think this is the right thing to do...\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We can choose any number of PCs to extract from our features\n",
    "# We will want at least enough to describe the variation in the dataset\n",
    "# But not so many that we also pick up noise\n",
    "n_components = 10\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_coeffs = pca.fit_transform(coeffs[keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb308de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot a heatmap of the PCA coefficients\n",
    "\"\"\"\n",
    "from scale_morphology.scales import plotting\n",
    "\n",
    "plotting.heatmap(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pairplot of PCA coefficients\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "\n",
    "# Get the right number of colours\n",
    "# Don't worry about how this works it's a stupid hack\n",
    "colours = list(TABLEAU_COLORS.values())[\n",
    "    : len(grouping_df.groupby(list(grouping_df.columns)))\n",
    "]\n",
    "\n",
    "plotting.pair_plot(pca_coeffs, grouping_df, colours, axis_label=\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f46189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run LDA\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Get labels for our different categories - we need to label each row in our dataframe\n",
    "# with which group it belongs to\n",
    "labels, uniques = pd.factorize(grouping_df.apply(lambda row: tuple(row.values), axis=1))\n",
    "\n",
    "# Now we cannot choose how many components to use in our dimensionality reduction;\n",
    "# LDA just finds the best (N-1) axes to distinguish our N classes.\n",
    "# Technically we could use any number less than N-1, but we want to keep all of them\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_coeffs = lda.fit_transform(pca_coeffs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot LDA heatmap of axes and pairplot\"\"\"\n",
    "import importlib; importlib.reload(plotting)\n",
    "plotting.heatmap((pca.components_.T @ lda.scalings_).T)\n",
    "\n",
    "plotting.pair_plot(lda_coeffs, grouping_df, colours, axis_label=\"LDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run k-fold validation to check the stability of the LDA\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
    "\n",
    "\n",
    "def k_fold_lda(input_coeffs: np.ndarray, grouping_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Run LDA on 5 folds of the given data and print the accuracy report\n",
    "    \"\"\"\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "lda_ = LinearDiscriminantAnalysis()\n",
    "\n",
    "scores = cross_val_score(\n",
    "        lda_, lda_coeffs, labels, cv=cv, scoring=\"balanced_accuracy\"\n",
    "    )\n",
    "print(f\"Balanced accuracy: {scores.mean():.3f} Â± {scores.std():.3f}\")\n",
    "\n",
    "preds = cross_val_predict(lda_, lda_coeffs, labels, cv=cv)\n",
    "print(classification_report(labels, preds, target_names=[str(u) for u in uniques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot LDA effect size - how much there\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fold_eta_sq = []\n",
    "for train_idx, test_idx in cv.split(lda_coeffs, labels):\n",
    "    lda_fold = LinearDiscriminantAnalysis().fit(\n",
    "        lda_coeffs[train_idx], labels[train_idx]\n",
    "    )\n",
    "    ld_test = lda_fold.transform(lda_coeffs[test_idx])\n",
    "    f_vals, _ = f_classif(ld_test, labels[test_idx])\n",
    "\n",
    "    df_between = len(np.unique(labels[test_idx])) - 1\n",
    "    df_within = len(test_idx) - len(np.unique(labels[test_idx]))\n",
    "    eta_sq = (df_between * f_vals) / (df_between * f_vals + df_within)\n",
    "    fold_eta_sq.append(eta_sq)\n",
    "\n",
    "fold_eta_sq = np.stack(fold_eta_sq)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(\n",
    "    np.arange(fold_eta_sq.shape[1]),\n",
    "    fold_eta_sq.mean(axis=0),\n",
    "    yerr=fold_eta_sq.std(axis=0),\n",
    "    capsize=5,\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.set_xlabel(\"LDA Axis\")\n",
    "ax.set_ylabel(r\"Effect Size ($\\eta^2$)\")\n",
    "ax.set_title(\"LDA Effect Size by Axis\")\n",
    "ax.set_xticks(np.arange(fold_eta_sq.shape[1]))\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
