{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a5e758",
   "metadata": {},
   "source": [
    "Scale Analysis Example\n",
    "====\n",
    "Using scales from Rabia Sevil, which is why this file is named like it is\n",
    "\n",
    "We'll read the LIF files, try to segment them out and then run EFA to summarise their shape variation.\n",
    "So far I've only done this on the ALP scales, since they looked like they'd be easiest to segment.\n",
    "\n",
    "Read in the scales\n",
    "----\n",
    "I've segmented the scales out following the process in [the segmentation notebook](segmentation.ipynb).\n",
    "\n",
    "This involved:\n",
    " - making an initial rough segmentation by thresholding\n",
    " - Running SAM (a transformer-based model from Meta) on the scales, using the rough segmentation as a prior\n",
    " - manually tidying them up where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in the images + various segmentations\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "import tifffile\n",
    "\n",
    "segmentation_dir = pathlib.Path(\"segmentation\")\n",
    "assert segmentation_dir.is_dir()\n",
    "\n",
    "dirs = [\n",
    "    segmentation_dir / \"images\",\n",
    "    segmentation_dir / \"mask_priors\",\n",
    "    segmentation_dir / \"sam_masks\",\n",
    "    segmentation_dir / \"cleaned_masks\",\n",
    "]\n",
    "\n",
    "names, images, rough_segmentations, sam_segmentations, clean_segmentations = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "for img_path, rough_path, sam_path, clean_path in zip(\n",
    "    *(sorted(list(d.glob(\"*.tif\"))) for d in dirs)\n",
    "):\n",
    "    name = img_path.name\n",
    "    assert name == rough_path.name == sam_path.name == clean_path.name\n",
    "\n",
    "    images.append(tifffile.imread(img_path))\n",
    "    rough_segmentations.append(tifffile.imread(rough_path))\n",
    "    sam_segmentations.append(tifffile.imread(sam_path))\n",
    "    clean_segmentations.append(tifffile.imread(clean_path))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ac7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "# G\n",
    "for axis, i in zip(axes.flat, (0, 90, 180, 270)):\n",
    "    axis.imshow(images[i])\n",
    "    axis.set_title(\"\\n\".join(textwrap.wrap(names[i], width=20)), fontsize=8)\n",
    "    axis.set_axis_off()\n",
    "fig.suptitle(\"This is what a scale looks like\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show a couple of the segmentations at different stages on top of the actual images\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_masks(masks, title):\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(6, 6))\n",
    "    for axis, img, mask in zip(axes.flat, images, masks):\n",
    "        axis.imshow(img, cmap=\"grey\")\n",
    "        axis.imshow(mask, alpha=0.5, cmap=\"Reds\")\n",
    "        axis.set_axis_off()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "plot_masks(rough_segmentations, \"First we threshold to get a rough idea of the scale shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(sam_segmentations, \"Then we use the SAM model to refine the prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(clean_segmentations, \"Then I went through by hand and cleaned them up a little in places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5796e8",
   "metadata": {},
   "source": [
    "Elliptical Fourier Analysis\n",
    "----\n",
    "We'll summarise their shapes using Elliptical Fourier Analysis (EFA)\n",
    "<a name=\"cite_ref-1\"></a><sup>[1]</sup>\n",
    "<a name=\"cite_ref-2\"></a><sup>[2]</sup>,\n",
    "which basically decomposes the boundary into sums of ellipses.\n",
    "The coefficients (strength and direction of each size of ellipse) tell us about the shape of the object.\n",
    "There's a demonstration of how this works [here](https://reinvantveer.github.io/2019/07/12/elliptical_fourier_analysis.html).\n",
    "\n",
    "Our edge is constructed as:\n",
    "\n",
    "\\begin{aligned}\n",
    "x(t) &= a_0 + \\sum_{n=1}^{N} \\big[a_n \\cos(n t) + b_n \\sin(n t)\\big],\\\\\n",
    "y(t) &= c_0 + \\sum_{n=1}^{N} \\big[c_n \\cos(n t) + d_n \\sin(n t)\\big],\n",
    "\\qquad t \\in [0, 2\\pi].\n",
    "\\end{aligned}\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{aligned}\n",
    "a_0 = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} x(t)\\,dt,\\qquad\n",
    "c_0 = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} y(t)\\,dt.\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "a_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} x(t)\\cos(n t)\\,dt, &\n",
    "b_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} x(t)\\sin(n t)\\,dt,\\\\\n",
    "c_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} y(t)\\cos(n t)\\,dt, &\n",
    "d_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} y(t)\\sin(n t)\\,dt.\n",
    "\\end{aligned}\n",
    "\n",
    "possibly up to some factors of $2\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First tidy the masks up a little, because I broke some of them when cleaning them\n",
    "\"\"\"\n",
    "\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from scale_morphology.scales.segmentation import largest_connected_component\n",
    "\n",
    "masks = [\n",
    "    255 * largest_connected_component(binary_fill_holes(m)).astype(np.uint8)\n",
    "    for m in clean_segmentations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40641cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform EFA on the scales and plot the reconstruction\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scale_morphology.scales import efa, errors, segmentation\n",
    "\n",
    "\n",
    "n_edge_points = 200\n",
    "order = 50\n",
    "\n",
    "coeffs = []\n",
    "for scale in tqdm(masks):\n",
    "    try:\n",
    "        coeffs.append(efa.coefficients(scale, n_edge_points, order))\n",
    "    except errors.BadImgError as e:\n",
    "        coeffs.append(np.ones((order, 4)) * np.nan)\n",
    "        print(f\"\\nError processing scale: {e}. NaN coeffs\")\n",
    "coeffs = np.stack(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c530ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "images = np.array(masks)\n",
    "flat_coeffs = coeffs.reshape((coeffs.shape[0], -1))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed_coeffs = np.ascontiguousarray(pca.fit_transform(flat_coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_morphology.scales import dashboard\n",
    "\n",
    "embeddable_imgs = [\n",
    "    dashboard.embeddable_image(i.astype(np.uint8)) for i in tqdm(clean_segmentations)\n",
    "]\n",
    "\n",
    "\n",
    "colours = []\n",
    "\n",
    "\n",
    "def _colour(name):\n",
    "    name = name.lower()\n",
    "    if \"hom\" in name:\n",
    "        if \"ontogenetic\" in name:\n",
    "            return \"Hom Onto\"\n",
    "        return \"Hom Regen\"\n",
    "    if \"ontogenetic\" in name:\n",
    "        return \"WT Onto\"\n",
    "    return \"WT Regen\"\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    colours.append(str(_colour(name)))\n",
    "\n",
    "dashboard_df = pd.DataFrame(transformed_coeffs, columns=[\"x\", \"y\"])\n",
    "dashboard_df[\"image\"] = embeddable_imgs\n",
    "dashboard_df[\"colour\"] = colours\n",
    "dashboard_df[\"name\"] = names\n",
    "\n",
    "dashboard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a4e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, save\n",
    "from bokeh.models import ColumnDataSource, HoverTool, CategoricalColorMapper\n",
    "from bokeh.resources import INLINE\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "factors = np.unique(dashboard_df[\"colour\"])\n",
    "mapper = CategoricalColorMapper(factors=factors, palette=f\"Category10_4\")\n",
    "\n",
    "datasource = ColumnDataSource(dashboard_df)\n",
    "fig = figure(\n",
    "    title=\"Test\", width=800, height=800, tools=\"pan, wheel_zoom, box_zoom, reset\"\n",
    ")\n",
    "\n",
    "fig.add_tools(\n",
    "    (\n",
    "        HoverTool(\n",
    "            tooltips=\"\"\"\n",
    "<div>\n",
    "    <div>\n",
    "        <img src=\"@image\" style=\"float: left; margin: 5px 5px 5px 5px;\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <span style=\"font-size: 17px; font-weight: bold;\">@name</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.scatter(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    source=datasource,\n",
    "    size=4,\n",
    "    color={\"field\": \"colour\", \"transform\": mapper},\n",
    "    legend_field=\"colour\",\n",
    ")\n",
    "\n",
    "for i, colour_value in enumerate(np.unique(dashboard_df[\"colour\"])):\n",
    "    group_points = dashboard_df[dashboard_df[\"colour\"] == colour_value][\n",
    "        [\"x\", \"y\"]\n",
    "    ].values\n",
    "\n",
    "    if len(group_points) >= 3:\n",
    "        hull = ConvexHull(group_points)\n",
    "\n",
    "        vertices = group_points[hull.vertices]\n",
    "\n",
    "        # Close the polygon by adding the first point at the end\n",
    "        vertices = np.vstack([vertices, vertices[0]])\n",
    "\n",
    "        hull_color = mapper.palette[i % len(mapper.palette)]\n",
    "        fig.patch(\n",
    "            x=vertices[:, 0],\n",
    "            y=vertices[:, 1],\n",
    "            alpha=0.2,\n",
    "            line_color=hull_color,\n",
    "            line_width=2,\n",
    "            fill_color=hull_color,\n",
    "        )\n",
    "\n",
    "filename = \"test_dashboard.html\"\n",
    "save(\n",
    "    fig,\n",
    "    filename=filename,\n",
    "    title=pathlib.Path(filename.replace(\".html\", \"\")).name,\n",
    "    resources=INLINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_morphology.scripts.plotting import interpret_dimensions\n",
    "\n",
    "interpret_dimensions._plot_pca_importance(\n",
    "    flat_coeffs, np.zeros(flat_coeffs.shape[0], dtype=bool)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [np.sum(m) / 255 for m in tqdm(np.array(masks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sizes, transformed_coeffs[:, 0], \".\")\n",
    "plt.xlabel(\"Scale Size (pixels?)\")\n",
    "plt.ylabel(\"PC1\")\n",
    "plt.title(\"The first principal component corresponds tells us about size\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_dir = pathlib.Path(\"rabia\")\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(plot_dir /\"sizes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576aad49",
   "metadata": {},
   "source": [
    "[^1](#cite_ref-1):  F. P. Kuhl and C. R. Giardina, ‘Elliptic Fourier features of a closed contour’, Computer Graphics and Image Processing, vol. 18, no. 3, pp. 236–258, Mar. 1982, doi: 10.1016/0146-664x(82)90034-x. \n",
    "\n",
    "[^2](#cite_ref-2): N. MacLeod, 'PalaeoMath 101 part 25: the centre cannot hold II: Elliptic fourier\n",
    "analysis.' Palaeontol. Assoc. Newslett. 79, 29–43, 2012 http://go.palass.org/65a."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
