{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a5e758",
   "metadata": {},
   "source": [
    "Scale Analysis Example\n",
    "====\n",
    "Using scales from Rabia Sevil, which is why this file is named like it is\n",
    "\n",
    "We'll read the LIF files, try to segment them out and then run EFA to summarise their shape variation.\n",
    "So far I've only done this on the ALP scales, since they looked like they'd be easiest to segment.\n",
    "\n",
    "Research Question\n",
    "----\n",
    "#### What morphological differences are there between hom/wt and onto/regen scales?\n",
    "\n",
    "NB:\n",
    " - hom = mutant (their bones form far quicker than normal, or something)\n",
    " - wt = wildtype, as they appear in the wild\n",
    " - onto = fully formed scales\n",
    " - regen = scales that are still growing\n",
    "\n",
    "we'd expect the hom scales to be weird shapes, the regen scales to be smaller...\n",
    "\n",
    "Read in the scales\n",
    "----\n",
    "I've segmented the scales out following the process in [the segmentation notebook](segmentation.ipynb).\n",
    "\n",
    "This involved:\n",
    " - making an initial rough segmentation by thresholding\n",
    " - Running SAM (a transformer-based model from Meta) on the scales, using the rough segmentation as a prior\n",
    " - manually tidying them up where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in the images + various segmentations\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "import tifffile\n",
    "\n",
    "segmentation_dir = pathlib.Path(\"segmentation\")\n",
    "assert segmentation_dir.is_dir()\n",
    "\n",
    "dirs = [\n",
    "    segmentation_dir / \"images\",\n",
    "    segmentation_dir / \"mask_priors\",\n",
    "    segmentation_dir / \"sam_masks\",\n",
    "    segmentation_dir / \"cleaned_masks\",\n",
    "]\n",
    "\n",
    "names, images, rough_segmentations, sam_segmentations, clean_segmentations = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "for img_path, rough_path, sam_path, clean_path in zip(\n",
    "    *(sorted(list(d.glob(\"*.tif\"))) for d in dirs)\n",
    "):\n",
    "    name = img_path.name\n",
    "    assert name == rough_path.name == sam_path.name == clean_path.name\n",
    "\n",
    "    images.append(tifffile.imread(img_path))\n",
    "    rough_segmentations.append(tifffile.imread(rough_path))\n",
    "    sam_segmentations.append(tifffile.imread(sam_path))\n",
    "    clean_segmentations.append(tifffile.imread(clean_path))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ac7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "# Pick some different looking scales\n",
    "for axis, i in zip(axes.flat, (0, 90, 180, 270)):\n",
    "    axis.imshow(images[i])\n",
    "    axis.set_title(\"\\n\".join(textwrap.wrap(names[i], width=20)), fontsize=8)\n",
    "    axis.set_axis_off()\n",
    "fig.suptitle(\"This is what a scale looks like\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show a couple of the segmentations at different stages on top of the actual images\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_masks(masks, title):\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(6, 6))\n",
    "    for axis, img, mask in zip(axes.flat, images, masks):\n",
    "        axis.imshow(img, cmap=\"grey\")\n",
    "        axis.imshow(mask, alpha=0.5, cmap=\"Reds\")\n",
    "        axis.set_axis_off()\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_masks(\n",
    "    rough_segmentations, \"First we threshold to get a rough idea of the scale shape\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(sam_segmentations, \"Then we use the SAM model to refine the prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masks(\n",
    "    clean_segmentations,\n",
    "    \"Then I went through by hand and cleaned them up a little in places\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5796e8",
   "metadata": {},
   "source": [
    "Elliptical Fourier Analysis\n",
    "----\n",
    "We'll summarise their shapes using Elliptical Fourier Analysis (EFA)\n",
    "<a name=\"cite_ref-1\"></a><sup>[1]</sup>\n",
    "<a name=\"cite_ref-2\"></a><sup>[2]</sup>,\n",
    "which basically decomposes the boundary into sums of ellipses.\n",
    "The coefficients (strength and direction of each size of ellipse) tell us about the shape of the object.\n",
    "There's a demonstration of how this works [here](https://reinvantveer.github.io/2019/07/12/elliptical_fourier_analysis.html).\n",
    "\n",
    "Our edge is constructed as:\n",
    "\n",
    "\\begin{aligned}\n",
    "x(t) &= a_0 + \\sum_{n=1}^{N} \\big[a_n \\cos(n t) + b_n \\sin(n t)\\big],\\\\\n",
    "y(t) &= c_0 + \\sum_{n=1}^{N} \\big[c_n \\cos(n t) + d_n \\sin(n t)\\big],\n",
    "\\qquad t \\in [0, 2\\pi].\n",
    "\\end{aligned}\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{aligned}\n",
    "a_0 = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} x(t)\\,dt,\\qquad\n",
    "c_0 = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} y(t)\\,dt.\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "a_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} x(t)\\cos(n t)\\,dt, &\n",
    "b_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} x(t)\\sin(n t)\\,dt,\\\\\n",
    "c_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} y(t)\\cos(n t)\\,dt, &\n",
    "d_n &= \\frac{1}{\\pi}\\int_{0}^{2\\pi} y(t)\\sin(n t)\\,dt.\n",
    "\\end{aligned}\n",
    "\n",
    "possibly up to some factors of $2\\pi$ or something.\n",
    "\n",
    "The $a_0$ and $c_0$ coefficients tell us about the locus/centroid of the object - i.e., its centre - which we don't care about here, since the relative position of the scale doesn't matter (we only care about its shape). We therefore only use the coefficients starting from $a_1$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First tidy the masks up a little, because I broke some of them when cleaning them\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from scale_morphology.scales.segmentation import largest_connected_component\n",
    "\n",
    "masks = [\n",
    "    255 * largest_connected_component(binary_fill_holes(m)).astype(np.uint8)\n",
    "    for m in clean_segmentations\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40641cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform EFA on the scales and plot the reconstruction\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scale_morphology.scales import efa, errors, segmentation\n",
    "\n",
    "\n",
    "n_edge_points = 100\n",
    "order = 30\n",
    "\n",
    "coeffs = []\n",
    "for scale in tqdm(masks):\n",
    "    try:\n",
    "        coeffs.append(efa.coefficients(scale, n_edge_points, order))\n",
    "    except errors.BadImgError as e:\n",
    "        coeffs.append(np.ones((order, 4)) * np.nan)\n",
    "        print(f\"\\nError processing scale: {e}. NaN coeffs\")\n",
    "coeffs = np.stack(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb82cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_morphology.scales import plotting\n",
    "\n",
    "i = 100\n",
    "fig, axis = plt.subplots(figsize=(8, 8))\n",
    "axis.imshow(images[i].sum(axis=2).T, origin=\"lower\", cmap=\"grey\")\n",
    "axis.set_aspect(\"equal\")\n",
    "\n",
    "locus = np.mean(np.where(masks[i] > 0), axis=1)\n",
    "plotting.plot_efa(\n",
    "    locus,\n",
    "    coeffs[i],\n",
    "    label=\"Elliptic Expansion best fit\",\n",
    "    linewidth=3,\n",
    "    color=\"#ff00fa\",\n",
    "    axis=axis,\n",
    ")\n",
    "\n",
    "x, y = efa.points_around_edge(masks[i], n_edge_points)\n",
    "axis.plot(x, y, \"#00ff05\", markersize=3, label=\"Edges\", marker=\"o\", linestyle=\"none\")\n",
    "\n",
    "axis.set_axis_off()\n",
    "axis.legend()\n",
    "\n",
    "fig.suptitle(\"Scale, edge points and reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c530ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "images = np.array(masks)\n",
    "flat_coeffs = coeffs.reshape((coeffs.shape[0], -1))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed_coeffs = np.ascontiguousarray(pca.fit_transform(flat_coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf455512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _colour(name):\n",
    "    name = name.lower()\n",
    "    if \"hom\" in name:\n",
    "        if \"ontogenetic\" in name:\n",
    "            return \"Hom Onto\"\n",
    "        return \"Hom Regen\"\n",
    "    if \"ontogenetic\" in name:\n",
    "        return \"WT Onto\"\n",
    "    return \"WT Regen\"\n",
    "\n",
    "\n",
    "colours = []\n",
    "for name in names:\n",
    "    colours.append(str(_colour(name)))\n",
    "colours = np.array(colours)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "for c in np.unique(colours):\n",
    "    axis.scatter(*transformed_coeffs[colours == c].T, label=c)\n",
    "\n",
    "axis.legend()\n",
    "axis.set_xlabel(f\"PC1 ({100*pca.explained_variance_ratio_[0]:.2f}% variance)\")\n",
    "axis.set_ylabel(f\"PC2 ({100*pca.explained_variance_ratio_[1]:.2f}% variance)\")\n",
    "fig.suptitle(\"PCA\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_morphology.scripts.plotting import interpret_dimensions\n",
    "from IPython.display import Image\n",
    "\n",
    "interpret_dimensions._plot_pca_importance(\n",
    "    flat_coeffs, np.zeros(flat_coeffs.shape[0], dtype=bool)\n",
    ")\n",
    "Image(filename=\"../../output/interpretation/importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [np.sum(m) / 255 for m in tqdm(np.array(masks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sizes, transformed_coeffs[:, 0], \".\")\n",
    "plt.xlabel(\"Scale Size (pixels?)\")\n",
    "plt.ylabel(\"PC1\")\n",
    "plt.title(\"The first principal component corresponds tells us about size\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_dir = pathlib.Path(\"rabia\")\n",
    "plot_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(plot_dir / \"sizes.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fd22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret_dimensions._correlation_plot(transformed_coeffs, coeffs)\n",
    "# Image(filename=\"../../output/interpretation/correlation.png\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e865f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_to_plot = 12\n",
    "# fig, axes = plt.subplots(1, 2)\n",
    "#\n",
    "# for axis, component in zip(axes, pca.components_):\n",
    "#     axis.bar(np.arange(n_to_plot), component[:n_to_plot])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da2e59",
   "metadata": {},
   "source": [
    "LDA\n",
    "----\n",
    "LDA is better for classification than PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots()\n",
    "idx = np.arange(flat_coeffs.shape[1])\n",
    "\n",
    "val = 0.05\n",
    "vars = flat_coeffs.var(axis=0)\n",
    "cutoff = 1000\n",
    "\n",
    "axis.bar(idx[(vars < val) & (vars < cutoff)], vars[(vars < val) & (vars < cutoff)])\n",
    "axis.bar(idx[(vars > val) & (vars < cutoff)], vars[(vars > val) & (vars < cutoff)])\n",
    "axis.set_ylabel(\"variance\")\n",
    "axis.set_title(\"Do we want to drop low-variance harmonics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f75fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_coeffs = lda.fit_transform(flat_coeffs, colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "ld_names = [f\"LD{i}\" for i in range(1, lda_coeffs.shape[1] + 1)]\n",
    "explained_variance = lda.explained_variance_ratio_\n",
    "\n",
    "ld_df = pd.DataFrame(lda_coeffs, columns=ld_names)\n",
    "ld_df[\"colour\"] = colours\n",
    "ld_df[\"name\"] = [x.strip(\".tif\") for x in names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for axis, (i, j), (k, var) in zip(\n",
    "    axes, combinations(range(1, 4), 2), enumerate(explained_variance)\n",
    "):  # 3 choose 2\n",
    "    for cls in np.unique(colours):\n",
    "        d = ld_df[ld_df[\"colour\"] == cls]\n",
    "        x, y = f\"LD{i}\", f\"LD{j}\"\n",
    "        axis.scatter(d[x], d[y], s=25, label=cls)\n",
    "\n",
    "        axis.set_xlabel(f\"{x} ({100*explained_variance[i-1]:.1f}% variance)\")\n",
    "        axis.set_ylabel(f\"{y} ({100*explained_variance[j-1]:.1f}% variance)\")\n",
    "\n",
    "fig.suptitle(f\"Separation accuracy: {100*lda.score(flat_coeffs, colours):.1f}%\")\n",
    "axes[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471da519",
   "metadata": {},
   "source": [
    "But what do these axes mean?\n",
    "----\n",
    "Ideally we want to understand intuitively what it means for LD1 to be the best axis for discrimination - e.g. does this mean the the Hom Onto scales are bigger/flatter/lumpier/... etc.?\n",
    "\n",
    "We can do this in two ways:\n",
    " 1. Analytically: by looking at the components of the LDA axes, we might be able to work out what they mean (e.g. if they correspond to the $a_0$ and $d_0$ coefficients, this means they're size or circularness or something). This is hard\n",
    " 2. Empirically: we can take a grid of points along our LDA axes, project them back into the original coefficient space and then draw the shapes that these correspond to. This means we should be able to see by eye what each axis corresponds to.\n",
    "\n",
    "### Analytically\n",
    "The LD axes are linear combinations of the EFA coefficient axes - we'll plot the strength of the components of each EFA coefficient in our LD axes to see if anything jumps out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "im = axis.imshow(np.abs(lda.scalings_).T, aspect=\"auto\", norm=LogNorm())\n",
    "fig.savefig(\"\")\n",
    "axis.set_xlabel(\"EFA Coefficient N\")\n",
    "axis.set_ylabel(\"Importance for LDA Project\")\n",
    "\n",
    "axis.set_yticks(range(3), (f\"LDC{i}\" for i in range(1, 4)))\n",
    "\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.ax.set_ylabel(\"Importance\", rotation=-90)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Only the first few EFA coefficients are important for separating the classes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839635bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 4\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 6), sharex=True)\n",
    "\n",
    "for i, (axis, strengths) in enumerate(zip(axes, lda.scalings_.T)):\n",
    "    p = axis.bar(np.arange(n_components), strengths[:n_components], width=0.2)\n",
    "    axis.bar_label(p, label_type=\"edge\")\n",
    "    axis.set_xlabel(\"\")\n",
    "    axis.set_xticks(range(4), [f\"${x}_1$\" for x in \"abcd\"])\n",
    "    axis.axhline(0, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "axes[2].set_xlabel(\"EFA Coeff\")\n",
    "fig.supylabel(\"Contribution to LDA axes\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Only the coefficients $b_0$ and $c_0$ contribute to separating the scale shapes\"\n",
    ")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f90b57",
   "metadata": {},
   "source": [
    "What does this mean?\n",
    "----\n",
    "We can work out what this means by making some \"pure\" axes of just these things:\n",
    " 1. Component 1 - almost equal contribution from $b_1$ and $c_1$\n",
    " 2. Component 2 - 6:1 ratio of contributions from $b_1$ and $c_1$\n",
    " 3. Component 3 - 1:-1 ratio of contributions from $b_1$ and $c_1$\n",
    "\n",
    "Going back to our definitions, this means our components are:\n",
    "\n",
    " 1. $x = k sin(t), y = k cos(t) \\implies x^2 + y^2 = k^2 \\implies$ a circle (clockwise)\n",
    " 2. $x = 3k sin(t), y = 0.5k cos(t) \\implies x^2 + 36y^2 = 9k \\implies$ an ellipse with eccentricity $\\frac{\\sqrt{35}}{6}$\n",
    " 3. $x = 0.5k sin(t), y = -0.5k cos(t) \\implies 4x^2 + 4y^2 = k^2 \\implies$ a smaller circle (now anticlockwise)\n",
    "\n",
    "If we draw these out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make some \"toy\" coefficients only up to first-order to illustrate the difference between these shapes\n",
    "\"\"\"\n",
    "\n",
    "from pyefd import reconstruct_contour\n",
    "\n",
    "# Define our axes\n",
    "# Scaled to have the same approximate relative magnitude as we found\n",
    "axis_1 = np.array([[0, 1, 1, 0]])\n",
    "axis_2 = np.array([[0, 3, 0.5, 0]])\n",
    "axis_3 = np.array([[0, 0.5, -0.5, 0]])\n",
    "\n",
    "# We'll make 8 example shapes, for each of the quadrants\n",
    "toy_coeffs = []\n",
    "co_ords = []\n",
    "indices = [-1, 0, 1]\n",
    "for i in indices:\n",
    "    for j in indices:\n",
    "        for k in indices:\n",
    "            toy_coeffs.append(i * axis_1 + j * axis_2 + k * axis_3)\n",
    "            co_ords.append(np.array([i, j, k]))\n",
    "\n",
    "# Convert these to shapes\n",
    "contours = [reconstruct_contour(c) for c in toy_coeffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcf370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot them\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    ABC.DEF.GHI\n",
    "    JKL.MNO.PQR\n",
    "    STU.VWX.YZ1\n",
    "    \"\"\",\n",
    "    figsize=(8, 4),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "for axis, c, co_ord in zip(axes.values(), contours, co_ords):\n",
    "    axis.scatter(*c.T, s=0.5)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(co_ord)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b6b3a",
   "metadata": {},
   "source": [
    "Empirically\n",
    "----\n",
    "Instead of trying to work out the shapes analytically, we can instead project our axes back into EFD coefficients and plot the resulting shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Turn our EFD axes back into shapes - see what they look like.\n",
    "\"\"\"\n",
    "\n",
    "axis_1, axis_2, axis_3 = lda.scalings_.T\n",
    "\n",
    "# We'll make 8 example shapes, for each of the quadrants\n",
    "toy_coeffs = []\n",
    "co_ords = []\n",
    "indices = [-1, 0, 1]\n",
    "for i in indices:\n",
    "    for j in indices:\n",
    "        for k in indices:\n",
    "            toy_coeffs.append(\n",
    "                np.array([i * axis_1 + j * axis_2 + k * axis_3]).reshape((-1, 4))\n",
    "            )\n",
    "            co_ords.append(np.array([i, j, k]))\n",
    "\n",
    "# Convert these to shapes\n",
    "contours = [reconstruct_contour(c) for c in toy_coeffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    ABC.DEF.GHI\n",
    "    JKL.MNO.PQR\n",
    "    STU.VWX.YZ1\n",
    "    \"\"\",\n",
    "    figsize=(8, 4),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "for axis, c, co_ord in zip(axes.values(), contours, co_ords):\n",
    "    axis.scatter(*c.T, s=0.5)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(co_ord)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99a538",
   "metadata": {},
   "source": [
    "Ok it's pretty hard to interpret these.\n",
    "\n",
    "Instead, we'll find the average of each class along the LDA axes, back-project these into EFD coefficients and find what shapes they represent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the mean of each class along the LDA axes\n",
    "\"\"\"\n",
    "\n",
    "classes = np.unique(colours)\n",
    "class_coords = {c: lda_coeffs[colours == c].mean(axis=0) for c in classes}\n",
    "\n",
    "fig, axis = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "for name, c in class_coords.items():\n",
    "    axis.scatter(*c, label=name)\n",
    "\n",
    "axis.legend()\n",
    "fig.suptitle(\"Class averages in LDA space\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project these back into EFD coefficients\n",
    "\"\"\"\n",
    "\n",
    "lda.scalings_.shape, [v.shape for v in class_coords.values()]\n",
    "\n",
    "projected_classes = [\n",
    "    np.dot(lda.scalings_, x).reshape(30, 4) for x in class_coords.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for axis, efd_coeffs, name in zip(axes.flat, projected_classes, classes):\n",
    "    plotting.plot_efa((0, 0), efd_coeffs, axis=axis, marker=\".\")\n",
    "    axis.set_aspect(\"equal\")\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576aad49",
   "metadata": {},
   "source": [
    "[^1](#cite_ref-1):  F. P. Kuhl and C. R. Giardina, ‘Elliptic Fourier features of a closed contour’, Computer Graphics and Image Processing, vol. 18, no. 3, pp. 236–258, Mar. 1982, doi: 10.1016/0146-664x(82)90034-x. \n",
    "\n",
    "[^2](#cite_ref-2): N. MacLeod, 'PalaeoMath 101 part 25: the centre cannot hold II: Elliptic fourier\n",
    "analysis.' Palaeontol. Assoc. Newslett. 79, 29–43, 2012 http://go.palass.org/65a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c230c0f",
   "metadata": {},
   "source": [
    "Appendix 1 - dashboard\n",
    "----\n",
    "It might be interesting to plot a dashboard showing the dimensionality reduction interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83971290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_morphology.scales import dashboard\n",
    "\n",
    "dashboard.write_dashboard(\n",
    "    lda_coeffs[:, 0:2],\n",
    "    images,\n",
    "    colours,\n",
    "    names,\n",
    "    \"test_dashboard.html\",\n",
    "    \"LDA Components 1&2\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
