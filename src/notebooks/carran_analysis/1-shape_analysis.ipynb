{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02feb135",
   "metadata": {},
   "source": [
    "Shape Analysis\n",
    "====\n",
    "This notebook runs through the shape analysis; the segmentation needs to already have been performed.\n",
    "\n",
    "We can do this using two dimensionality-reduction techniques: **PCA** or **LDA**.\n",
    "\n",
    "1 - PCA\n",
    "----\n",
    "PCA is a standard dimensionality reduction technique that is often used on high-dimensional data.\n",
    "\n",
    "<details>\n",
    "<summary>High dimensional data?</summary>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Our Elliptic Fourier Analysis represents each scale as a list of numbers; the length of this list is the dimensionality of our space.  \n",
    "\n",
    "Here, \"dimensionality\" just means \"degrees of of freedom\" - how many independent parameters we have describing each scale.  \n",
    "\n",
    "If we use 200 numbers to describe the shape of each scale, then our Elliptic Fourier Space is 200-dimensional.\n",
    "\n",
    "Applying PCA or LDA to these high-dimensional lists of numbers will find ways to represent them as lower-dimensional (read: shorter) lists of numbers.\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "PCA returns the \"principal components\" (PCs) in our space - the axes along which the dataset varies most.\n",
    "PCA is a powerful technique because it is:\n",
    "1. **A linear transformation**: the PCs are an interpretable combination of the original features\n",
    "2. **Based on global variance**: they capture the dominant patterns of variation across the whole dataset.\n",
    "\n",
    "We need point 1 for our shape analysis - we want our lists of numbers to be interpretable in terms of biological features (e.g. size, aspect ratio, ...).\n",
    "\n",
    "Point 2 means that PCA gives the best low-dimensional summary of the *whole* dataset - this is not necessarily the same thing as the low-dimensional summary that best separates between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, read in the data\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "parent_dir = pathlib.Path(\n",
    "    \"~/zebrafish_rdsf/Carran/Postgrad/segmentations_cleaned\"\n",
    ").expanduser()\n",
    "assert parent_dir.exists()\n",
    "\n",
    "segmentation_paths = sorted([str(x) for x in parent_dir.glob(\"*.tif\")])\n",
    "f\"{len(segmentation_paths)} segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in metadata, including image filepaths\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scale_morphology.scales import metadata\n",
    "\n",
    "df = metadata.df([str(x) for x in segmentation_paths])\n",
    "df = df.drop(columns=\"no_scale\")\n",
    "\n",
    "assert len(df) == 928, \"Did the number of scales change?\"\n",
    "print(len(df), \"scales after dropping empty ones\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If necessary, read in the scales from the RDSF and perform EFA on them.\n",
    "\n",
    "Otherwise read in the EFA coefficients from a cache\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scale_morphology.scales import efa\n",
    "\n",
    "\n",
    "coeff_dump = pathlib.Path(\"carran_coeffs.npy\")\n",
    "\n",
    "if coeff_dump.is_file():\n",
    "    coeffs = np.load(coeff_dump)\n",
    "else:\n",
    "    n_edge_points, order = 300, 50\n",
    "    coeffs = efa.run_analysis(\n",
    "        df[\"path\"],\n",
    "        df[\"magnification\"],\n",
    "        n_points=n_edge_points,\n",
    "        order=order,\n",
    "        n_threads=32,\n",
    "    )\n",
    "\n",
    "    np.save(coeff_dump, coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bff9bc",
   "metadata": {},
   "source": [
    "Now that we've found the EFD descriptors for our scales, we can perform PCA to find which features are most important for describing the variation in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform PCA\n",
    "\n",
    "We won't scale the features before PCA, because we want them to keep their original\n",
    "magnitudes - if we scale the features to have a standard mean/std, then we will\n",
    "artificially inflate the importance of the higher harmonics.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We can choose any number of PCs to extract from our features\n",
    "# We will want at least enough to describe the variation in the dataset\n",
    "n_components = 10\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_coeffs = pca.fit_transform(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fa60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualise the PCA coeffs\"\"\"\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pca_barplot(scalings):\n",
    "    \"\"\"\n",
    "    Make a bar plot showing the contribution of the features to each PC\n",
    "    \"\"\"\n",
    "    # Want to plot the first two features (size, d_1) and then a whole number\n",
    "    # of ellipses, each of which are made up of 4 components\n",
    "    n_ellipses = 3\n",
    "    n_bars = 2 + n_ellipses * 4\n",
    "\n",
    "    n_pcs = scalings.shape[0]\n",
    "\n",
    "    # 4 columns, with as many rows as we need\n",
    "    n_col = 4\n",
    "    n_row = int(np.ceil(n_pcs / n_col))\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(n_col * 4, n_row * 4))\n",
    "\n",
    "    # First two ticks for size, d_1; then calculate the rest + build their labels\n",
    "    xticks = [-3, -1]\n",
    "    xticklabels = [\"size\", r\"$d_1$\"]\n",
    "    vlines = [-2, 0]\n",
    "    for i in range(n_ellipses):\n",
    "        xticklabels += [rf\"$a_{i+2}$\", rf\"$b_{i+2}$\", rf\"$c_{i+2}$\", rf\"$d_{i+2}$\"]\n",
    "\n",
    "        start, end = 1 + 5 * i, 5 + 5 * i\n",
    "        xticks += list(range(start, end))\n",
    "\n",
    "        vlines.append(start - 1)\n",
    "\n",
    "    for i, (axis, scaling) in enumerate(zip(axes.flat, scalings)):\n",
    "        axis.bar(xticks, scaling[:n_bars])\n",
    "\n",
    "        axis.set_xticks(\n",
    "            xticks,\n",
    "            xticklabels,\n",
    "            ha=\"right\",\n",
    "        )\n",
    "\n",
    "        # Every 4 features is one ellipse - separate them visually\n",
    "        for v in vlines:\n",
    "            axis.axvline(v, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "        axis.set_ylim(-1.1, 1.1)\n",
    "        axis.set_yticks([-1, 0, 1])\n",
    "\n",
    "        axis.set_title(f\"PC{i+1}\")\n",
    "\n",
    "\n",
    "pca_barplot(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f213196f",
   "metadata": {},
   "source": [
    "We can see here that the first two PCs correspond almost purely to the size and aspect ratio (coefficient $d_1$) respectively.\n",
    "\n",
    "This might be expected - the size of each scale varies much more than the other features, so it makes sense that this coefficient explains a lot of the global variation.\n",
    "\n",
    "We can also plot a heatmap of the features' contribution to the PCs; this is a slightly more compact way of visualsing the above bar charts. We will notice that the higher features (corresponding to smaller spatial scales) contribute less to the early PCs.\n",
    "\n",
    "We can also plot the feature importance of each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762842ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot heatmap of components in terms of features, and importance of components\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "def heatmap(scalings):\n",
    "    \"\"\"\n",
    "    Plot a heatmap showing component loadings in terms of features\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    im = axis.matshow(\n",
    "        scalings.T,\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"seismic\",\n",
    "        norm=colors.CenteredNorm(vcenter=0.0),\n",
    "    )\n",
    "\n",
    "    n_components = scalings.shape[0]\n",
    "    axis.set_xticks(range(n_components), range(1, n_components + 1))\n",
    "    axis.set_title(\"Feature contribution to each component\")\n",
    "    axis.set_ylabel(\"Component\")\n",
    "    axis.set_xlabel(\"Feature\")\n",
    "    fig.colorbar(im, ax=axis)\n",
    "\n",
    "\n",
    "def feature_importance(estimator: BaseEstimator):\n",
    "    \"\"\"\n",
    "    Plot feature importance of PCA or LDA\n",
    "    \"\"\"\n",
    "    fig, axis = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    percent_importances = estimator.explained_variance_ratio_\n",
    "    axis.bar(np.arange(len(percent_importances)), percent_importances)\n",
    "    axis.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "heatmap(pca.components_)\n",
    "feature_importance(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b581872",
   "metadata": {},
   "source": [
    "We can see that the feature importance is dominated by the size feature (#0). I will change this in a bit to do the PCA without size...\n",
    "\n",
    "However, when we make a scatter colour-coding our different classes, we will find that PCA does not distinguish between them very well. This is to be expected; PCA finds the components that most describe **global** variation, not variation between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872af33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot 2D KDE/scatter plots showing the different classes, along the different PC axes\n",
    "\"\"\"\n",
    "\n",
    "from scale_morphology.scales import plotting\n",
    "\n",
    "keep = df[\"sex\"] != \"?\"\n",
    "grouping_df = df.loc[keep, [\"sex\", \"age\"]]\n",
    "grouping_colours = [\"royalblue\", \"cornflowerblue\", \"brown\", \"red\", \"blue\", \"indianred\", \"lightcoral\"]\n",
    "\n",
    "plotting.pair_plot(\n",
    "    pca_coeffs[keep],\n",
    "    grouping_df,\n",
    "    grouping_colours,\n",
    "    axis_label=\"PC\",\n",
    "    normalise=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda77df1",
   "metadata": {},
   "source": [
    "Linear Discriminant analysis (LDA) will be much better than PCA at separating our classes.\n",
    "\n",
    "Linear Discriminant Analysis\n",
    "----\n",
    "\n",
    "### PCA vs LDA\n",
    "PCA is a dimensionality-reduction method that uses the directions of greatest \n",
    "variance within our dataset as the principal components.\n",
    "PCA is an unsupervised method - we don't need to tell PCA about the class labels\n",
    "(which makes sense - it described global variation).\n",
    "It is a powerful method, but it isn't guaranteed that the directions of greatest global\n",
    "variance will also separate between our classes.\n",
    "It is likely, if the separation between the classes is based on the same meaningful features\n",
    "that describe the whole dataset, but it isn't guaranteed.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) however is a classifier - it can be used for dimensionality\n",
    "reduction, but instead of finding the directions of greatest overall variance it finds the directions\n",
    "which separate best between our classes.\n",
    "It is a supervised method - we need to tell it what our class labels are so it can separate between them.\n",
    "If we have $N$ classes, we can find at most $N-1$ directions to separate them.\n",
    "We can use LDA as a drop-in replacement for the PCA above, and make the same plots: a bar chart\n",
    "showing our LDA axes in terms of the EFD features, and scatter plots/histograms showing how the classes\n",
    "are separated.\n",
    "\n",
    "<details>\n",
    "<summary>Maths</summary>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "LDA is a classifier that creates linear decision boundaries (as you might expect from the name)\n",
    "by fitting the conditional probabilities of each class to the densities and using Bayes' rule.\n",
    "\n",
    "It finds the a covariance matrix and the sample means of each class,\n",
    "then solves an eigenvalue problem to find the linear discriminant directions.\n",
    "\n",
    "It is an optimal classifier if the classes are distributed as Gaussians with different means\n",
    "but a shared covariance matrix.\n",
    "It is a sub-optimal (but still fine) classifier if the classes are not distributed as Gaussians.\n",
    "If they do not share a covariance matrix, the true best discriminant boundaries will be non-linear;\n",
    "this is a problem for LDA since it can only find linear decision boundaries (Quadratic Discriminant\n",
    "Analysis can deal with the case when the classes have different covariance matrices, but does not\n",
    "give us nice interpretable axes of greatest variation).\n",
    "\n",
    "The assumption that the covariance matrices for each are equal is therefore quite strong - we should\n",
    "probably formally check this in each case...\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform LDA on the above categories, pair plot + show that it separates better\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def lda(\n",
    "    input_coeffs: np.ndarray, label_df: pd.DataFrame\n",
    ") -> tuple[LinearDiscriminantAnalysis, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform Linear Discriminant Analysis on a set of high_dimensional\n",
    "    `input_coeffs`, given a dataframe holding the columns we want to\n",
    "    label by `label_df`.\n",
    "\n",
    "    Returns the fitted estimator + the transformed coeffs\n",
    "    \"\"\"\n",
    "    assert len(label_df) == input_coeffs.shape[0]\n",
    "\n",
    "    labels, uniques = pd.factorize(\n",
    "        label_df.apply(lambda row: tuple(row.values), axis=1)\n",
    "    )\n",
    "\n",
    "    # Now we cannot choose how many components to use in our dimensionality reduction;\n",
    "    # LDA just finds the best (N-1) axes to distinguish our N classes.\n",
    "    # Technically we could use any number less than N-1, but we want to keep all of them\n",
    "    lda_ = LinearDiscriminantAnalysis()\n",
    "    return lda_, lda_.fit_transform(input_coeffs, labels)\n",
    "\n",
    "\n",
    "_, lda_coeffs = lda(coeffs[keep], grouping_df)\n",
    "\n",
    "plotting.pair_plot(\n",
    "    lda_coeffs, grouping_df, grouping_colours, axis_label=\"LD\", normalise=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6e24a",
   "metadata": {},
   "source": [
    "We can see that, as before, we have reduced the dimension of the space and can see some separation between the categories.\n",
    "The separation is better than it was before - we can also run the LDA and PCA using different groupings, to see this effect even more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ade744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose labels + plot colours\n",
    "keep = (df[\"sex\"] == \"M\") & (df[\"age\"] == 18)\n",
    "grouping_df = df.loc[keep, [\"growth\", \"mutation\"]]\n",
    "grouping_colours = [\"indianred\", \"lightblue\", \"red\", \"blue\"]\n",
    "\n",
    "# PCA plot - don't need to re-fit PCA, since the PCA\n",
    "# is independent of grouping\n",
    "plotting.pair_plot(\n",
    "    pca_coeffs[keep],\n",
    "    grouping_df,\n",
    "    grouping_colours,\n",
    "    axis_label=\"PC\",\n",
    "    normalise=True,\n",
    ")\n",
    "\n",
    "# Do new LDA fit and plot the axes\n",
    "lda_, lda_coeffs = lda(coeffs[keep], grouping_df)\n",
    "plotting.pair_plot(\n",
    "    lda_coeffs, grouping_df, grouping_colours, axis_label=\"LD\", normalise=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee735b",
   "metadata": {},
   "source": [
    "We can see that, even though the LDA only defines a few axes compared to the PCA, it is much better at separating out our classes.\n",
    "\n",
    "However, we have not been very rigorous here.\n",
    "It is possible that the LDA algorithm is fitting to noise - we might suspect this if our LD vectors contain large contributions from\n",
    "high EFA harmonics (recall that the PCA vectors were mostly comprised of the lower harmonics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA scalings are transposed wrt PCA\n",
    "heatmap(lda_.scalings_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ccdb2",
   "metadata": {},
   "source": [
    "We can see that, not only are the LDA components largely composed of the higher harmonics, the coefficients are huge (see the colourbar)\n",
    "which suggests that we might be overfitting to noise in our high harmonics.\n",
    "\n",
    "Let's see whether the exact same pipeline works if our \"classes\" are freshly generated random integers - if we are indeed overfitting,\n",
    "then the LDA will be even able so separate our classes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a new dataframe holding just random numbers, separate these classes with LDA and plot the result\n",
    "\"\"\"\n",
    "\n",
    "# Make a df with a single column holding random ints\n",
    "rng = np.random.default_rng()\n",
    "grouping_df = pd.DataFrame({\"random\": rng.integers(0, 4, size=np.sum(keep))})\n",
    "\n",
    "lda_, lda_coeffs = lda(coeffs[keep], grouping_df)\n",
    "plotting.pair_plot(\n",
    "    lda_coeffs, grouping_df, grouping_colours, axis_label=\"LD\", normalise=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3bb08",
   "metadata": {},
   "source": [
    "We can see that, even though there should be nothing to distinguish our classes, we can perfectly separate them using LDA.\n",
    "\n",
    "This is because the LDA axes are very noisy, as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b938eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(lda_.scalings_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708f01f",
   "metadata": {},
   "source": [
    "A more formal and rigorous way to show this might be to run k-fold validation on our LDA classifier,\n",
    "which basically just runs the LDA repeatedly on subsets of the data and checks that the results are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65704d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run k-fold validation on our LDA\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
    "\n",
    "\n",
    "def k_fold_lda(input_coeffs: np.ndarray, label_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Run LDA on 5 folds of the given data and print the accuracy report\n",
    "    \"\"\"\n",
    "    assert len(label_df) == input_coeffs.shape[0]\n",
    "\n",
    "    labels, uniques = pd.factorize(\n",
    "        label_df.apply(lambda row: tuple(row.values), axis=1)\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    lda_ = LinearDiscriminantAnalysis()\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        lda_, input_coeffs, labels, cv=cv, scoring=\"balanced_accuracy\"\n",
    "    )\n",
    "    print(f\"Balanced accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "    preds = cross_val_predict(lda_, input_coeffs, labels, cv=cv)\n",
    "    print(classification_report(labels, preds, target_names=[str(u) for u in uniques]))\n",
    "\n",
    "    fold_eta_sq = []\n",
    "    for train_idx, test_idx in cv.split(input_coeffs, labels):\n",
    "        lda_fold = LinearDiscriminantAnalysis().fit(input_coeffs[train_idx], labels[train_idx])\n",
    "        ld_test = lda_fold.transform(input_coeffs[test_idx])\n",
    "        f_vals, _ = f_classif(ld_test, labels[test_idx])\n",
    "\n",
    "        df_between = len(np.unique(labels[test_idx])) - 1\n",
    "        df_within = len(test_idx) - len(np.unique(labels[test_idx]))\n",
    "        eta_sq = (df_between * f_vals) / (df_between * f_vals + df_within)\n",
    "        fold_eta_sq.append(eta_sq)\n",
    "\n",
    "    fold_eta_sq = np.stack(fold_eta_sq)\n",
    "    print(f\"LD etasq score mean ± std:\")\n",
    "    for x in fold_eta_sq.T:\n",
    "        print(\"\\t\", x.mean(), \"+-\", x.std())\n",
    "\n",
    "\n",
    "\n",
    "k_fold_lda(coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d015c",
   "metadata": {},
   "source": [
    "With four equally sized categories, we'd expect chance performance of around 25% accuracy (which is approximately what we get).\n",
    "\n",
    "This is bad - it's telling us that our LDA isn't fitting to signal, it's fitting to noise.\n",
    "\n",
    "Using PCA then LDA\n",
    "----\n",
    "Earlier, I mentioned that PCA is likely to pick out meaningful features since it finds the axes of greatest global variation.\n",
    "\n",
    "We can exploit this to make the LDA more stable - first we do PCA, then we run LDA on the PCA transformed coefficients.\n",
    "This will avoid the overfitting issue, since by truncating the PCs we feed to the LDA algorithm we should retain most of the\n",
    "signal while discarding the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34052b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run LDA on the PCA-transformed coefficients\n",
    "\"\"\"\n",
    "\n",
    "keep = (df[\"sex\"] == \"M\") & (df[\"age\"] == 18)\n",
    "grouping_df = df.loc[keep, [\"growth\", \"mutation\"]]\n",
    "grouping_colours = [\"indianred\", \"lightblue\", \"red\", \"blue\"]\n",
    "\n",
    "# Do new LDA fit, this time to the PCA coeffs, and plot the axes\n",
    "lda_, lda_coeffs = lda(pca_coeffs[keep], grouping_df)\n",
    "plotting.pair_plot(\n",
    "    lda_coeffs, grouping_df, grouping_colours, axis_label=\"LD\", normalise=True\n",
    ")\n",
    "\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2904b10",
   "metadata": {},
   "source": [
    "Our separation is not as good as when we fed all the features into the LDA, but our performance is still better than random chance.\n",
    "\n",
    "We can plot a heatmap of the LDA axes projected onto the original features... TODO fill this in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce24297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Project from LDA -> PCA -> features and show the heatmap\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def lda2pca2features(pca_scalings: np.ndarray, lda_scalings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The PCA and LDA loadings, return a 2d array giving the LDA loadings\n",
    "    in terms of the original features\n",
    "    \"\"\"\n",
    "    return pca_scalings.T @ lda_scalings\n",
    "\n",
    "heatmap(lda2pca2features(pca.components_, lda_.scalings_).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237dafd0",
   "metadata": {},
   "source": [
    "The LDA loadings are now primarily in the lower features, which is what one would expect since these are what make up the PCA components.\n",
    "\n",
    "These are the features which separate our classes best.\n",
    "\n",
    "To interpret what they mean in terms of biological features (i.e. shape), we can plot the outline of selected scales on the scatter plots, for both PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.shape_plot(lda_coeffs, scale_paths=df.loc[keep, \"path\"].values, axis_label=\"LD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bcd83",
   "metadata": {},
   "source": [
    "And LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10875742",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.shape_plot(pca_coeffs, scale_paths=df[\"path\"], axis_label=\"PC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70aac4d",
   "metadata": {},
   "source": [
    "We can run this analysis with many different labellings; some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"sex\"] != \"?\") & (df[\"growth\"] == np.inf) & (df[\"mutation\"] == \"WT\")\n",
    "grouping_df = df.loc[keep, [\"sex\", \"age\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ee0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"sex\"] != \"?\") & (df[\"mutation\"] == \"WT\") & (df[\"growth\"] == 10.0)\n",
    "grouping_df = df.loc[keep, [\"sex\", \"age\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = df[\"sex\"] != \"?\"\n",
    "grouping_df = df.loc[keep, [\"sex\", \"age\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"sex\"] == \"M\") & (df[\"age\"] == 18)\n",
    "grouping_df = df.loc[keep, [\"growth\", \"mutation\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"mutation\"] == \"WT\") & (df[\"sex\"] == \"F\")\n",
    "grouping_df = df.loc[keep, [\"age\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"mutation\"] == \"WT\") & (df[\"sex\"] == \"M\")\n",
    "grouping_df = df.loc[keep, [\"age\"]]\n",
    "k_fold_lda(pca_coeffs[keep], grouping_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
