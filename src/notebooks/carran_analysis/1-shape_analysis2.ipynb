{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02feb135",
   "metadata": {},
   "source": [
    "Shape Analysis\n",
    "====\n",
    "This notebook runs through the shape analysis once the segmentation has been performed.\n",
    "\n",
    "First we'll get some stats on the metadata: growth stages, sex, age, mutation etc. from the filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "parent_dir = pathlib.Path(\n",
    "    \"~/zebrafish_rdsf/Carran/Postgrad/segmentations_cleaned\"\n",
    ").expanduser()\n",
    "assert parent_dir.exists()\n",
    "\n",
    "segmentation_paths = sorted([str(x) for x in parent_dir.glob(\"*.tif\")])\n",
    "f\"{len(segmentation_paths)} segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scale_morphology.scales import metadata\n",
    "\n",
    "df = metadata.df([str(x) for x in segmentation_paths])\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop scales with missing data\n",
    "\"\"\"\n",
    "\n",
    "df = df[~df[\"no_scale\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If necessary, read in the scales from the RDSF and perform EFA on them.\n",
    "\n",
    "Otherwise read in the EFA coefficients from a cache\n",
    "\"\"\"\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.measure import euler_number\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "\n",
    "from scale_morphology.scales import efa\n",
    "from scale_morphology.scales.segmentation import largest_connected_component\n",
    "\n",
    "\n",
    "def load_scale_data(segmentation_path):\n",
    "    \"\"\"\n",
    "    Returns the cleaned segmentations\n",
    "    \"\"\"\n",
    "    scale = tifffile.imread(segmentation_path)\n",
    "    if euler_number(scale) != 1:\n",
    "        # Fill holes\n",
    "        scale = binary_fill_holes(scale)\n",
    "        # Remove small objects\n",
    "        scale = (largest_connected_component(scale) * 255).astype(np.uint8)\n",
    "\n",
    "        # It's possible we might have removed everything, so just make sure we haven't here\n",
    "        if euler_number(scale) != 1:\n",
    "            raise ValueError(f\"Got {euler_number(scale)=}\")\n",
    "\n",
    "    return scale\n",
    "\n",
    "\n",
    "coeff_dump = pathlib.Path(\"carran_coeffs.npy\")\n",
    "\n",
    "if coeff_dump.is_file():\n",
    "    coeffs = np.load(coeff_dump)\n",
    "\n",
    "else:\n",
    "    n_edge_points, order = 300, 50\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        scales = np.array(\n",
    "            list(\n",
    "                tqdm(\n",
    "                    executor.map(load_scale_data, df[\"path\"]),\n",
    "                    total=len(df),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # The default magnification is 4.0\n",
    "    magnifications = df[\"magnification\"]\n",
    "    magnifications[pd.isna(magnifications)] = 4.0\n",
    "\n",
    "    coeffs = [\n",
    "        efa.coefficients(scale, n_edge_points, order, magnification=4 / magnification)\n",
    "        for scale, magnification in zip(tqdm(scales), magnifications)\n",
    "    ]\n",
    "    coeffs = np.stack(coeffs)\n",
    "    np.save(coeff_dump, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff450a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run PCA on the coefficients and plot the contributions\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def _heatmap(scalings):\n",
    "    fig, axis = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "    vmax = np.max(np.abs(scalings))\n",
    "    im = axis.imshow(\n",
    "        scalings.T,\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"seismic\",\n",
    "        norm=colors.CenteredNorm(vcenter=0.0),\n",
    "    )\n",
    "    axis.set_yticks(range(scalings.shape[1]))\n",
    "    fig.colorbar(im, ax=axis)\n",
    "\n",
    "\n",
    "def _pca_barplot(scalings):\n",
    "    n_bars = min(14, scalings.shape[1])\n",
    "    fig, axes = plt.subplots(1, scalings.shape[1], figsize=(scalings.shape[1] * 4, 4))\n",
    "\n",
    "    # First two ticks for size, d_1; then calculate the rest + build their labels\n",
    "    xticks = [-3, -1]\n",
    "    xticklabels = [\"size\", r\"$d_1$\"]\n",
    "    vlines = [-2, 0]\n",
    "    for i in range(n_bars // 4):\n",
    "        xticklabels += [rf\"$a_{i+2}$\", rf\"$b_{i+2}$\", rf\"$c_{i+2}$\", rf\"$d_{i+2}$\"]\n",
    "\n",
    "        start, end = 1 + 5 * i, 5 + 5 * i\n",
    "        xticks += list(range(start, end))\n",
    "\n",
    "        vlines.append(start - 1)\n",
    "\n",
    "    for axis, scaling in zip(axes, scalings.T, strict=True):\n",
    "        axis.bar(xticks, scaling[:n_bars])\n",
    "\n",
    "        axis.set_xticks(\n",
    "            xticks,\n",
    "            xticklabels,\n",
    "            ha=\"right\",\n",
    "        )\n",
    "\n",
    "        # Separate out the different conceptual bits\n",
    "        for v in vlines:\n",
    "            axis.axvline(v, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "        axis.set_ylim(-1.1, 1.1)\n",
    "\n",
    "\n",
    "def _plot_vectors(scalings: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plot eigenvectors for a dimensionality reduction thing\n",
    "    \"\"\"\n",
    "    _heatmap(scalings)\n",
    "    _pca_barplot(scalings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run LDA on the PCA coefficients, given some categories, and plot a scatter plot\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def clear2colour_cmap(colour) -> colors.Colormap:\n",
    "    \"\"\"\n",
    "    Colormap that varies from clear to a colour\n",
    "    \"\"\"\n",
    "    c_white = colors.colorConverter.to_rgba(\"white\", alpha=0)\n",
    "    c_black = colors.colorConverter.to_rgba(colour, alpha=0.5)\n",
    "    return colors.ListedColormap([c_white, c_black], f\"clear2{colour}\")\n",
    "\n",
    "\n",
    "def _plot_kde_scatter(\n",
    "    axis: plt.Axes,\n",
    "    x_coeffs: np.ndarray,\n",
    "    y_coeffs: np.ndarray,\n",
    "    lda_labels,\n",
    "    colours,\n",
    "    grouping_cols,\n",
    "    uniques,\n",
    ") -> list[Patch]:\n",
    "    \"\"\"\n",
    "    Plot a scatter plot and LDA colour-coded using our different labels.\n",
    "\n",
    "    Returns handles for plotting\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(lda_labels)\n",
    "    assert len(unique_labels) == len(colours), f\"{len(unique_labels)=}, {len(colours)=}\"\n",
    "\n",
    "    # Grid for this pair\n",
    "    xmin, xmax = x_coeffs.min(), x_coeffs.max()\n",
    "    ymin, ymax = y_coeffs.min(), y_coeffs.max()\n",
    "    xx, yy = np.mgrid[xmin:xmax:200j, ymin:ymax:200j]\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "    pretty_labels = {\n",
    "        code: \", \".join(f\"{col}={val}\" for col, val in zip(grouping_cols, uniq))\n",
    "        for code, uniq in enumerate(uniques)\n",
    "    }\n",
    "\n",
    "    handles = []\n",
    "    for label, colour in zip(unique_labels, colours):\n",
    "        mask = lda_labels == label\n",
    "        if mask.sum() < 2:\n",
    "            raise ValueError(\"KDE needs at least 2 points\")\n",
    "\n",
    "        data = np.vstack([x_coeffs[mask], y_coeffs[mask]])\n",
    "        kde = gaussian_kde(data)\n",
    "        f = np.reshape(kde(positions).T, xx.shape)\n",
    "\n",
    "        axis.contourf(xx, yy, f, levels=15, cmap=clear2colour_cmap(colour))\n",
    "        n = int(np.sum(mask))\n",
    "        handles.append(Patch(color=colour, label=f\"{pretty_labels[label]}, {n=}\"))\n",
    "\n",
    "        axis.scatter(*data, color=colour, s=5, marker=\"s\", linewidth=0.5, edgecolor=\"k\")\n",
    "\n",
    "    return handles\n",
    "\n",
    "\n",
    "def _kde_scatter(uniques, lda_coeffs, lda_labels, colours, grouping_cols):\n",
    "    n_axes = lda_coeffs.shape[1] - 1\n",
    "    fig, axes = plt.subplots(1, n_axes, figsize=(5 * n_axes, 5))\n",
    "    if n_axes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, axis in enumerate(axes):\n",
    "        handles = _plot_kde_scatter(\n",
    "            axis,\n",
    "            lda_coeffs[:, i],\n",
    "            lda_coeffs[:, i + 1],\n",
    "            lda_labels,\n",
    "            colours,\n",
    "            grouping_cols,\n",
    "            uniques,\n",
    "        )\n",
    "\n",
    "    handles = [\n",
    "        Patch(color=h.get_facecolor(), label=f\"{h.get_label()}\") for h in handles\n",
    "    ]\n",
    "    axes[0].legend(handles=handles)\n",
    "\n",
    "\n",
    "def _plot_extrema(coeffs: np.ndarray, paths: pd.Series):\n",
    "    \"\"\"\n",
    "    Show some scale images on the scatter plots\n",
    "    \"\"\"\n",
    "    n_axes = coeffs.shape[1] - 1\n",
    "    fig, axes = plt.subplots(1, n_axes, figsize=(4 * n_axes, 4))\n",
    "    if n_axes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # directions - we'll dot with these to get the extrema\n",
    "    angles = np.deg2rad(np.arange(0, 360, 22.5))\n",
    "    dirns = np.stack([np.cos(angles), np.sin(angles)], axis=1)\n",
    "\n",
    "    for i, axis in enumerate(axes):\n",
    "        co_ords = np.stack([coeffs[:, i], coeffs[:, i + 1]])\n",
    "        axis.scatter(*co_ords, s=2)\n",
    "\n",
    "        scores = np.linalg.matmul(co_ords.T, dirns.T)\n",
    "        extrema_idx = scores.argmax(axis=0)\n",
    "\n",
    "        extrema_locs = co_ords.T[extrema_idx]\n",
    "        extrema_paths = paths.values[extrema_idx]\n",
    "\n",
    "        extrema_imgs = [tifffile.imread(f)[::20, ::20] for f in extrema_paths]\n",
    "\n",
    "        for loc, img in zip(extrema_locs, extrema_imgs, strict=True):\n",
    "            ab = AnnotationBbox(\n",
    "                OffsetImage(img, zoom=0.3, cmap=clear2colour_cmap(\"k\")),\n",
    "                loc,\n",
    "                pad=0.05,\n",
    "                frameon=True,\n",
    "                bboxprops={\"edgecolor\": \"k\", \"linewidth\": 1},\n",
    "            )\n",
    "            axis.add_artist(ab)\n",
    "\n",
    "\n",
    "def plot_lda(\n",
    "    df: pd.DataFrame,\n",
    "    coeffs: np.ndarray,\n",
    "    grouping_cols: str | list[str],\n",
    "    colours: list[str],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform LDA on a dataframe using the values in columns.\n",
    "\n",
    "    Requires the coeffs to be in the same order as the df columns\n",
    "    \"\"\"\n",
    "    assert len(df) == len(coeffs), f\"{len(df)=} {coeffs.shape=}\"\n",
    "\n",
    "    # First, get an encoding for the grouping categories and their labels\n",
    "    if isinstance(grouping_cols, str):\n",
    "        grouping_cols = [grouping_cols]\n",
    "    combos = df[grouping_cols].apply(lambda row: tuple(row.values), axis=1)\n",
    "    lda_labels, uniques = pd.factorize(combos)\n",
    "\n",
    "    n_pcs = 10\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"pca\", PCA(n_components=n_pcs)),\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"lda\", LinearDiscriminantAnalysis()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    scores = cross_val_score(\n",
    "        pipeline, coeffs, lda_labels, cv=cv, scoring=\"balanced_accuracy\"\n",
    "    )\n",
    "\n",
    "    # Plot the contribution from each EFA coefficient to the PCA\n",
    "    pipeline.fit(coeffs, lda_labels)\n",
    "    _plot_vectors(pipeline.named_steps[\"pca\"].components_.T)\n",
    "    plt.gcf().suptitle(\"PCs\")\n",
    "\n",
    "    # Then plot KDE + scatter plots\n",
    "    _kde_scatter(\n",
    "        uniques, pipeline.transform(coeffs), lda_labels, colours, grouping_cols\n",
    "    )\n",
    "\n",
    "    # Plot a heatmap of LDA vectors in terms of PCA coefficients\n",
    "    lda = pipeline.named_steps[\"lda\"]\n",
    "    _heatmap(lda.scalings_)\n",
    "\n",
    "    # Plot extrema\n",
    "    _plot_extrema(pipeline.transform(coeffs), df[\"path\"])\n",
    "\n",
    "    print(f\"Crossval score: {scores.mean():.3f}\" \"\\u00b1\" f\"{scores.std():.3f}\")\n",
    "    print(classification_report(\n",
    "        lda_labels, pipeline.predict(coeffs), target_names=[str(u) for u in uniques]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = df[\"sex\"] != \"?\"\n",
    "plot_lda(\n",
    "    df[keep],\n",
    "    coeffs[keep],\n",
    "    [\"sex\", \"age\"],\n",
    "    colours=[\n",
    "        \"royalblue\",\n",
    "        \"cornflowerblue\",\n",
    "        \"brown\",\n",
    "        \"red\",\n",
    "        \"blue\",\n",
    "        \"indianred\",\n",
    "        \"lightcoral\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"sex\"] == \"M\") & (df[\"age\"] == 18)\n",
    "plot_lda(\n",
    "    df[keep],\n",
    "    coeffs[keep],\n",
    "    [\"growth\", \"mutation\"],\n",
    "    colours=[\"indianred\", \"lightblue\", \"red\", \"blue\"],\n",
    ")\n",
    "plt.gcf().suptitle(\"M18 only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"mutation\"] == \"WT\") & (df[\"sex\"] == \"F\")\n",
    "plot_lda(df[keep], coeffs[keep], \"age\", colours=[\"indianred\", \"lightcoral\", \"red\"])\n",
    "plt.gcf().suptitle(\"F WTs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = (df[\"mutation\"] == \"WT\") & (df[\"sex\"] == \"F\")\n",
    "plot_lda(df[keep], coeffs[keep], \"age\", colours=[\"indianred\", \"lightcoral\", \"red\"])\n",
    "plt.gcf().suptitle(\"M WTs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "copy_df[\"random\"] = rng.integers(0, 2, size=len(df))\n",
    "copy_df[\"random2\"] = rng.integers(0, 4, size=len(df))\n",
    "\n",
    "plot_lda(\n",
    "    copy_df,\n",
    "    coeffs,\n",
    "    [\"random\", \"random2\"],\n",
    "    colours=list(colors.TABLEAU_COLORS.keys())[:8],\n",
    ")\n",
    "plt.gcf().suptitle(\"Spurious groupings are unlikely with the entire dataset\")\n",
    "\n",
    "plot_lda(copy_df, coeffs, \"random2\", colours=list(colors.TABLEAU_COLORS.keys())[:4])\n",
    "keep = (df[\"sex\"] == \"M\") & (df[\"age\"] == 18)\n",
    "plot_lda(\n",
    "    copy_df[keep],\n",
    "    coeffs[keep],\n",
    "    [\"random\", \"random2\"],\n",
    "    colours=list(colors.TABLEAU_COLORS.keys())[:8],\n",
    ")\n",
    "plt.gcf().suptitle(\n",
    "    \"But if we only use a subset, it becomes quite easy to separate them based on random noise\"\n",
    ")\n",
    "\n",
    "plot_lda(\n",
    "    copy_df[keep],\n",
    "    coeffs[keep],\n",
    "    \"random2\",\n",
    "    colours=list(colors.TABLEAU_COLORS.keys())[:4],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
