{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df59372",
   "metadata": {},
   "source": [
    "Segmentation Illustration\n",
    "====\n",
    "I've settled on a hybrid pipeline - we first use classical techniques to get a rough mask.\n",
    "Then we use the bounding box of this rough mask + some points from within as a prior, and the Meta Segment Anything Model (SAM) to do the actual segmentation.\n",
    "\n",
    "Finally, manually tidied up these masks to get the final dataset that I worked with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0a101",
   "metadata": {},
   "source": [
    "Initial Segmentation\n",
    "----\n",
    "First we'll make some rough masks by doing a classical CV pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff56d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pathlib\n",
    "\n",
    "parent_dir = pathlib.Path(\"~/zebrafish_rdsf/Rabia/SOST scales\").expanduser()\n",
    "assert parent_dir.exists()\n",
    "\n",
    "scale_dirs = tuple(d for d in parent_dir.glob(\"*\") if not d.stem in {\".DS_Store\", \"TIFs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scale_morphology.scales import read\n",
    "\n",
    "# Just look at the ALP scales since they seem easiest to segment\n",
    "(alp_dir,) = (d for d in scale_dirs if \"ALP\" in d.stem)\n",
    "\n",
    "alp_files = list(alp_dir.glob(\"*.lif\"))\n",
    "print(len(alp_files), \"files\")\n",
    "\n",
    "names, images = [], []\n",
    "for path in alp_files:\n",
    "    name, img = zip(*read.read_lif(path))\n",
    "    names += name\n",
    "    images += img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert to greyscale\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def factor_int(n):\n",
    "    val = math.ceil(math.sqrt(n))\n",
    "    val2 = int(n / val)\n",
    "    while val2 * val != float(n):\n",
    "        val -= 1\n",
    "        val2 = int(n / val)\n",
    "    return val, val2\n",
    "\n",
    "\n",
    "def plot_imgs(images, **plot_kw):\n",
    "    n_figs = factor_int(len(images))\n",
    "\n",
    "    fig, axes = plt.subplots(*n_figs, figsize=[2 * x for x in n_figs])\n",
    "    for axis, img in zip(tqdm(axes.flat), images):\n",
    "        axis.imshow(img, **plot_kw, cmap=\"grey\")\n",
    "        axis.set_axis_off()\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_imgs(images[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "greyscale = [rgb2gray(i) for i in tqdm(images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blur to remove noise\n",
    "Contrast enhance\n",
    "Threshold\n",
    "Remove small objects\n",
    "Binary opening\n",
    "Fill holes\n",
    "Remove small objects\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_fill_holes, label\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.morphology import binary_opening, disk\n",
    "from skimage.filters import gaussian, threshold_minimum, threshold_mean\n",
    "\n",
    "\n",
    "def threshold(i):\n",
    "    return (i < threshold_minimum(i)) | (i < threshold_mean(i))\n",
    "\n",
    "\n",
    "def clear_border_keep_large(img):\n",
    "    \"\"\"\n",
    "    Clear border but dont do anything if it would remove too much\n",
    "    \"\"\"\n",
    "    sum_before = np.sum(img)\n",
    "    cleared = clear_border(img)\n",
    "\n",
    "    if np.sum(cleared) < 0.1 * sum_before:\n",
    "        print(\"large obj touching border\", file=sys.stderr)\n",
    "        return img\n",
    "    return cleared\n",
    "\n",
    "\n",
    "def _largest_connected_component(binary_array):\n",
    "    \"\"\"\n",
    "    Return the largest connected component of a binary array, as a binary array\n",
    "\n",
    "    :param binary_array: Binary array.\n",
    "    :returns: Largest connected component.\n",
    "\n",
    "    \"\"\"\n",
    "    labelled, _ = label(binary_array, np.ones((3, 3)))\n",
    "\n",
    "    # Find the size of each component\n",
    "    sizes = np.bincount(labelled.ravel())\n",
    "    sizes[0] = 0\n",
    "\n",
    "    retval = labelled == np.argmax(sizes)\n",
    "    return retval\n",
    "\n",
    "\n",
    "# Structuring element for binary opening\n",
    "elem = disk(10)\n",
    "\n",
    "blurred = [gaussian(i, sigma=3) for i in tqdm(greyscale, desc=\"Blurring\")]\n",
    "enhanced = [\n",
    "    equalize_adapthist(i, kernel_size=2001)\n",
    "    for i in tqdm(blurred, desc=\"Enhance contrast\")\n",
    "]\n",
    "thresholded = [threshold(i) for i in tqdm(enhanced, desc=\"thresholding\")]\n",
    "cleared = [\n",
    "    clear_border_keep_large(i) for i in tqdm(thresholded, desc=\"clearing borders\")\n",
    "]\n",
    "opened = [binary_opening(i, elem) for i in tqdm(cleared, desc=\"opening\")]\n",
    "filled = [binary_fill_holes(i) for i in tqdm(opened, desc=\"filling\")]\n",
    "\n",
    "final = [\n",
    "    _largest_connected_component(i) for i in tqdm(filled, desc=\"Removing small objs\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(final[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save them to disk, in case we need them later\n",
    "\"\"\"\n",
    "\n",
    "import tifffile\n",
    "\n",
    "out_dir = pathlib.Path(\"segmentation/\")\n",
    "\n",
    "mask_prior_dir = out_dir / \"mask_priors\"\n",
    "mask_prior_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for mask, name in zip(final, tqdm(names)):\n",
    "    tifffile.imwrite(mask_prior_dir / (name + \".tif\"), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Also save the originals\"\"\"\n",
    "\n",
    "img_dir = out_dir / \"images\"\n",
    "img_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for img, name in zip(images, tqdm(names)):\n",
    "    tifffile.imwrite(img_dir / (name + \".tif\"), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7a6a6",
   "metadata": {},
   "source": [
    "Transformer-based segmentation\n",
    "----\n",
    "Now we've got some masks that are almost there, we want to tidy things up a little.\n",
    "\n",
    "One could do this by hand but that would be extremely slow, and quite subjective. Instead, we can tidy the masks up, at least initally, by using SAM to segment out the scales.\n",
    "This doesn't \"just work\" out of the box, though - if we try to segment the scales without any prior, it will not give a better result than the above classical pipeline - it may also label bubbles, leeched stain, the different parts of the scale etc. as different objects, which we don't want.\n",
    "\n",
    "We can instead use the above masks as a prior for the SAM.\n",
    "We will build a bounding-box around the above segmentation masks, choose some points from within them, and feed these in to SAM to be used as a prior for the segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06867069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_erosion\n",
    "\n",
    "\n",
    "def bbox_from_mask(m, pad=16):\n",
    "    ys, xs = np.nonzero(m)\n",
    "    if ys.size == 0:\n",
    "        return None\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    return np.array(\n",
    "        [\n",
    "            max(0, x0 - pad),\n",
    "            max(0, y0 - pad),\n",
    "            min(m.shape[1] - 1, x1 + pad),\n",
    "            min(m.shape[0] - 1, y1 + pad),\n",
    "        ],\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_pos_points(m, n=4):\n",
    "    mm = binary_erosion(m, iterations=100)\n",
    "    ys, xs = np.nonzero(mm if mm.any() else m)\n",
    "\n",
    "    idx = np.linspace(0, ys.size - 1, num=min(n, ys.size)).astype(int)\n",
    "    pts = np.stack([xs[idx], ys[idx]], axis=1)\n",
    "    return pts\n",
    "\n",
    "\n",
    "boxes = [bbox_from_mask(i, pad=32) for i in tqdm(final)]\n",
    "points = [sample_pos_points(i, n=10) for i in tqdm(final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eead58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5)\n",
    "for axis, img, box, point in zip(axes.flat, final, boxes, points):\n",
    "    axis.imshow(img, cmap=\"binary\")\n",
    "\n",
    "    axis.axvline(box[0])\n",
    "    axis.axvline(box[2])\n",
    "\n",
    "    axis.axhline(box[1])\n",
    "    axis.axhline(box[3])\n",
    "\n",
    "    for pt in point:\n",
    "        axis.scatter(*pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b30f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "device = \"cuda\"\n",
    "model_type = \"vit_h\"\n",
    "sam_checkpoint = pathlib.Path(\"checkpoints\") / \"sam_vit_h_4b8939.pth\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device).eval()\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0374a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "sam_masks = []\n",
    "with torch.inference_mode():\n",
    "    for img, pts, box in zip(tqdm(enhanced), points, boxes):\n",
    "        grey = cv2.cvtColor(img.astype(np.float32), cv2.COLOR_GRAY2RGB)\n",
    "        predictor.set_image(grey)\n",
    "\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=pts,\n",
    "            point_labels=np.ones((pts.shape[0],), dtype=np.int32),\n",
    "            box=box,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "\n",
    "        sam_masks.append(masks[np.argmax(scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imgs(sam_masks[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "for axis, img, mask in zip(axes.flat, tqdm(enhanced), sam_masks):\n",
    "    axis.imshow(img, cmap=\"binary\")\n",
    "    axis.imshow(mask, alpha=0.5)\n",
    "    axis.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_mask_dir = out_dir / \"sam_masks\"\n",
    "sam_mask_dir.mkdir()\n",
    "\n",
    "for mask, name in zip(sam_masks, tqdm(names)):\n",
    "    tifffile.imwrite(sam_mask_dir / (name + \".tif\"), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77e994",
   "metadata": {},
   "source": [
    "Edit the masks manually\n",
    "----\n",
    "\n",
    "The SAM masks are still not perfect. Some (but not all) of them will need editing - the below cells will open a GUI that can be used to tidy up any of the masks that aren't sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c62b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [255 * tifffile.imread(f).astype(np.uint8) for f in mask_dir.glob(\"*.tif\")]\n",
    "old_masks = [\n",
    "    255 * tifffile.imread(f).astype(np.uint8)\n",
    "    for f in (pathlib.Path(\"segmentation_stuff\") / \"masks\").glob(\"*.tif\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from pathlib import Path\n",
    "\n",
    "mask_paths = sorted(mask_dir.glob(\"*.tif\"))\n",
    "img_paths = {p.name: img_dir / p.name for p in mask_paths}\n",
    "\n",
    "state = {\"i\": 0, \"viewer\": None, \"labels\": None}\n",
    "\n",
    "\n",
    "def load_index(i):\n",
    "    name = mask_paths[i].name\n",
    "    im = tifffile.imread(img_paths[name])\n",
    "    mask = tifffile.imread(mask_paths[i]).astype(np.uint8)\n",
    "\n",
    "    if state[\"labels\"] is None:\n",
    "        state[\"image\"] = viewer.add_image(im, name=\"image\")\n",
    "        state[\"labels\"] = viewer.add_labels(mask, name=\"mask\", opacity=0.5)\n",
    "    else:\n",
    "        state[\"image\"].data = im\n",
    "        state[\"labels\"].data = mask\n",
    "    viewer.title = f\"{i+1}/{len(mask_paths)} : {name}\"\n",
    "\n",
    "\n",
    "def save_current():\n",
    "    name = mask_paths[state[\"i\"]].name\n",
    "    out_path = out_dir / \"cleaned_masks\" / name\n",
    "    tifffile.imwrite(out_path, (state[\"labels\"].data > 0).astype(np.uint8) * 255)\n",
    "    print(f\"Saved {out_path}\")\n",
    "\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "state[\"viewer\"] = viewer\n",
    "\n",
    "\n",
    "@viewer.bind_key(\"s\")\n",
    "def _save(v):\n",
    "    save_current()\n",
    "\n",
    "\n",
    "@viewer.bind_key(\"n\")\n",
    "def _next(v):\n",
    "    save_current()\n",
    "    if state[\"i\"] < len(mask_paths) - 1:\n",
    "        state[\"i\"] += 1\n",
    "        load_index(state[\"i\"])\n",
    "\n",
    "\n",
    "@viewer.bind_key(\"p\")\n",
    "def _prev(v):\n",
    "    save_current()\n",
    "    if state[\"i\"] > 0:\n",
    "        state[\"i\"] -= 1\n",
    "        load_index(state[\"i\"])\n",
    "\n",
    "\n",
    "load_index(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale-morphology (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
